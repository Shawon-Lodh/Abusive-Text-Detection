{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Dataset_find_from_twitter.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"ni0aujo63bcr","executionInfo":{"status":"ok","timestamp":1606013311431,"user_tz":-360,"elapsed":887,"user":{"displayName":"shawon lodh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioQ-STcVJ6-N7WcYfRh3JP9m5K6pJgMUvNBf5B=s64","userId":"10496108193185418146"}}},"source":["#Import Library\n","import pandas as pd\n","import tweepy\n","import timeit\n","import io"],"execution_count":18,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Y8Ev_6FUzSzU"},"source":["#File read from Google drive\n","\n","\n","*   original dataset is -   **original_hatespeechtwitter.csv**\n","*   scraped dataset from twitter will be -  **extract_from_twitter_hatespeech_labels.csv** \n","*   Final dataset will be - **final_hatespeech_labels_modified.csv**\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"M7q37luD8Prj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606012366611,"user_tz":-360,"elapsed":22082,"user":{"displayName":"shawon lodh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioQ-STcVJ6-N7WcYfRh3JP9m5K6pJgMUvNBf5B=s64","userId":"10496108193185418146"}},"outputId":"57e6ad8b-943b-4bd9-8e8d-5d1e2be63035"},"source":["#upload dataset \n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"DGgaP0bJ8PsG","executionInfo":{"status":"ok","timestamp":1606013939226,"user_tz":-360,"elapsed":882,"user":{"displayName":"shawon lodh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioQ-STcVJ6-N7WcYfRh3JP9m5K6pJgMUvNBf5B=s64","userId":"10496108193185418146"}}},"source":["dataset = pd.read_csv('gdrive/My Drive/Abusive_text_detection/Dataset_collection/original_hatespeechtwitter.csv')\n","# print(dataset)"],"execution_count":24,"outputs":[]},{"cell_type":"code","metadata":{"id":"F4tD6b-R4ACN","executionInfo":{"status":"ok","timestamp":1606012484506,"user_tz":-360,"elapsed":856,"user":{"displayName":"shawon lodh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioQ-STcVJ6-N7WcYfRh3JP9m5K6pJgMUvNBf5B=s64","userId":"10496108193185418146"}}},"source":["#Dataset to list\n","dataframe_list_version = dataset.values.tolist()\n","# print(dataframe_list_version)"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"Da5OBh1i5C6J","colab":{"base_uri":"https://localhost:8080/"},"outputId":"6bf0715d-a859-4cf6-db62-216acdde3782"},"source":["###### twitter api connection ######\n","consumer_key = \"ixpRkVeXW55UlWGxNohERAkfV\"\n","consumer_secret = \"lKRXGZEo8UVmX9QyjYgOrwS5PgTLTBQwt9JLY0D7j0P9q3gbKy\"\n","access_key = \"888381043237048321-ONH1Mwv1bYLKlrTeuOzTRlDoRpzoDSr\"\n","access_secret = \"zPJMNBGyu576dQ2C10WE4tTFa8MX1ygC439sr35xhYITi\"\n","\n","# Authorization to consumer key and consumer secret\n","auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n","\n","# Access to user's access key and access secret\n","auth.set_access_token(access_key, access_secret)\n","\n","# Calling api\n","api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)\n","\n","sample = dataframe_list_version\n","\n","\n","tweet_id = []\n","tweet = []\n","label = []\n","label_value = []\n","\n","found_tweet_id = []\n","count_found_tweet_id = 0\n","\n","not_found_tweet_id = []\n","count_not_found_tweet_id = 0\n","\n","\n","for line in sample:\n","    try:\n","        tweets_post = api.get_status(id=str(line[0]))\n","        # print(tweets_post.text)\n","        tweet.append((tweets_post.text).replace('\\r',' ').replace('\\n',' '))\n","        label.append(line[1])\n","        if line[1] == 'abusive':\n","            label_value.append(1)\n","        else:\n","            label_value.append(0)\n","\n","\n","        tweet_id.append(line[0])\n","        found_tweet_id.append(line[0])\n","        count_found_tweet_id +=1\n","    except:\n","        # print(\"This id of tweets is Not found now\")\n","        not_found_tweet_id.append(line[0])\n","        count_not_found_tweet_id +=1\n","\n","\n","# print(len(found_tweet_id), len(not_found_tweet_id))\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Rate limit reached. Sleeping for: 4\n","Rate limit reached. Sleeping for: 762\n","Rate limit reached. Sleeping for: 367\n","Rate limit reached. Sleeping for: 363\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"bVyNU30x0PjQ","executionInfo":{"status":"ok","timestamp":1606013843862,"user_tz":-360,"elapsed":919,"user":{"displayName":"shawon lodh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioQ-STcVJ6-N7WcYfRh3JP9m5K6pJgMUvNBf5B=s64","userId":"10496108193185418146"}}},"source":["#Save collected dataset\n","data = {'tweet_id': tweet_id, 'tweet': tweet,'label': label, 'label_value': label_value}\n","data_frame = pd.DataFrame.from_dict(data)\n","data1 = {'Tweets': tweet,'abusive': label_value}\n","data_frame1 = pd.DataFrame.from_dict(data1)\n","# data_frame.to_csv('aother_dataset.csv',encoding = 'UTF-16',sep=\",\", index=False)\n","data_frame.to_csv('gdrive/My Drive/Abusive_text_detection/Dataset_collection/extract_from_twitter_hatespeech_labels.csv',index=False)\n","data_frame1.to_csv('gdrive/My Drive/Abusive_text_detection/Dataset_collection/final_hatespeechtwitter.csv',index=False)\n"],"execution_count":23,"outputs":[]}]}