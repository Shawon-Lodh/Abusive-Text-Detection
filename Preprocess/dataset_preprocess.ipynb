{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"dataset_preprocess.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyM02mNa9Bc9K9mUCJvbrrxt"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"wG9nKgOEMo99","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1605679998522,"user_tz":-360,"elapsed":12099,"user":{"displayName":"shawon lodh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioQ-STcVJ6-N7WcYfRh3JP9m5K6pJgMUvNBf5B=s64","userId":"10496108193185418146"}},"outputId":"48c71f80-c4bf-452c-85da-e1aa6bdbaaf4"},"source":["#this is needed for expand string constraction like can't -> can not,won't-> will not\n","!pip install contractions\n","#this is needed for convert text to number like three -> 3\n","!pip install text2num"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting contractions\n","  Downloading https://files.pythonhosted.org/packages/00/92/a05b76a692ac08d470ae5c23873cf1c9a041532f1ee065e74b374f218306/contractions-0.0.25-py2.py3-none-any.whl\n","Collecting textsearch\n","  Downloading https://files.pythonhosted.org/packages/42/a8/03407021f9555043de5492a2bd7a35c56cc03c2510092b5ec018cae1bbf1/textsearch-0.0.17-py2.py3-none-any.whl\n","Collecting Unidecode\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/42/d9edfed04228bacea2d824904cae367ee9efd05e6cce7ceaaedd0b0ad964/Unidecode-1.1.1-py2.py3-none-any.whl (238kB)\n","\u001b[K     |████████████████████████████████| 245kB 3.7MB/s \n","\u001b[?25hCollecting pyahocorasick\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f4/9f/f0d8e8850e12829eea2e778f1c90e3c53a9a799b7f412082a5d21cd19ae1/pyahocorasick-1.4.0.tar.gz (312kB)\n","\u001b[K     |████████████████████████████████| 317kB 28.9MB/s \n","\u001b[?25hBuilding wheels for collected packages: pyahocorasick\n","  Building wheel for pyahocorasick (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyahocorasick: filename=pyahocorasick-1.4.0-cp36-cp36m-linux_x86_64.whl size=81701 sha256=15881500bc70e404f9bc91f8fc28c258a8103156be338bf71fb29e26712e5d75\n","  Stored in directory: /root/.cache/pip/wheels/0a/90/61/87a55f5b459792fbb2b7ba6b31721b06ff5cf6bde541b40994\n","Successfully built pyahocorasick\n","Installing collected packages: Unidecode, pyahocorasick, textsearch, contractions\n","Successfully installed Unidecode-1.1.1 contractions-0.0.25 pyahocorasick-1.4.0 textsearch-0.0.17\n","Collecting text2num\n","  Downloading https://files.pythonhosted.org/packages/9e/ff/ba46a1ec72aa7c003b617e4fd03cdd23551064b261ebcea6b25722c4187e/text2num-2.2.1.tar.gz\n","Building wheels for collected packages: text2num\n","  Building wheel for text2num (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for text2num: filename=text2num-2.2.1-cp36-none-any.whl size=27338 sha256=a845bffc33751e52dbbdf3bdec084c23b3d0bcb1b853497fc6ef69feb9306e5d\n","  Stored in directory: /root/.cache/pip/wheels/44/df/80/e253ea78ba47fd987f259e301be3fa60352ebb932e575fb118\n","Successfully built text2num\n","Installing collected packages: text2num\n","Successfully installed text2num-2.2.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"5A6iWl04Mo-F","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1605680000980,"user_tz":-360,"elapsed":14536,"user":{"displayName":"shawon lodh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioQ-STcVJ6-N7WcYfRh3JP9m5K6pJgMUvNBf5B=s64","userId":"10496108193185418146"}},"outputId":"2a8910dc-d63a-495f-8548-7926ea4fc8d3"},"source":["#import library\n","import nltk\n","nltk.download('stopwords')\n","nltk.download('wordnet')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"code","metadata":{"id":"IkYyuOmkMo-K"},"source":["import json\n","import string\n","import contractions\n","from textblob import TextBlob\n","import unidecode\n","from text_to_num import alpha2digit\n","import pandas as pd\n","import re\n","from nltk.corpus import stopwords\n","from nltk.stem.wordnet import WordNetLemmatizer\n","from nltk.stem import PorterStemmer"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"srn_XU6H_vj7"},"source":["#Read dataset "]},{"cell_type":"code","metadata":{"id":"M7q37luD8Prj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1605680024241,"user_tz":-360,"elapsed":36160,"user":{"displayName":"shawon lodh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GioQ-STcVJ6-N7WcYfRh3JP9m5K6pJgMUvNBf5B=s64","userId":"10496108193185418146"}},"outputId":"fe1027bb-3c3a-4ee6-f892-886a6c0a349a"},"source":["#Read dataset\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"DGgaP0bJ8PsG"},"source":["dataset = pd.read_csv('gdrive/My Drive/Abusive_text_detection/Dataset_collection/final_hatespeechtwitter.csv')\n","# print(dataset)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5EoPGHDa8PsE"},"source":["# Preprocessing start(1 - 14)"]},{"cell_type":"code","metadata":{"id":"7xUniHyR8PsX"},"source":["#1\n","def convert_lower(text):\n","  return text.lower()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SNoXPZA9YLJy"},"source":["#2\n","def accented_char_to_ascii_char(text):\n","  return unidecode.unidecode(text)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uddyJvlFXvCR"},"source":["#3\n","def expand_contraction(text):\n","  return contractions.fix(text)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ODmC-3lddxT-"},"source":["#4\n","def number_words_to_numeric_form(text):\n","  return alpha2digit(text, \"en\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IKiVbQSAdxQQ"},"source":["#5\n","def remove_urls(text):\n","  return re.sub(r\"(http|https)(\\s*:\\s*)(\\s*//\\s*)\\S+|www\\.\\S+\",\"\",text)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SEwlSJk6YkJb"},"source":["#6\n","def remove_email(text):\n","  return re.sub(r\"([a-zA-Z0-9+._-]+(\\s*@\\s*)[a-zA-Z0-9._-]+\\.[a-zA-Z0-9_-]+)\",\"\",text)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"19wJ9485iajj"},"source":["#7\n","def remove_hashtag_and_usertag(text):\n","  return re.sub(r\"#\\s*\\S+|@\\s*\\S+\",\"\",text)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ItmHKjB2i-2f"},"source":["#8\n","def remove_rt_url_different_numeric_tag_numbers(text):\n","  return re.sub(r'rt|url|(\\w*\\d\\w*)', '', text)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6GMRvnzUyGW9"},"source":["#9\n","def remove_punctuation(text):\n","  return re.sub('[%s]' % re.escape(string.punctuation), '', text)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LI8abECB81DH"},"source":["#10\n","def remove_extra_white_spaces(text):\n","  return ' '.join(text.split())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rl-GfhJcStcl"},"source":["#11\n","def spelling_correction(text):\n","  return str(TextBlob(text).correct())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2ayygpwiNSc8"},"source":["#12\n","def lemmatization(text):\n","  return ' '.join([WordNetLemmatizer().lemmatize(word) for word in text.split()])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zN4kng9vIzNn"},"source":["#13\n","def stemming(text):\n","  return ' '.join([PorterStemmer().stem(word) for word in text.split()])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EjHXwKySL78B"},"source":["#14\n","def remove_stopword(text):\n","  return ' '.join([word for word in text.split() if word not in stopwords.words('english')])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"l0txxTm-O2YL"},"source":["def cleaning(dataset):\n","  corpus = list(dataset['Tweets'])\n","  # print(corpus)\n","  for i,m in enumerate(corpus):\n","    text = convert_lower(m)                                       #1\n","    text = accented_char_to_ascii_char(text)                      #2\n","    text = expand_contraction(text)                               #3\n","    text = number_words_to_numeric_form(text)                     #4\n","    text = remove_urls(text)                                      #5\n","    text = remove_email(text)                                     #6\n","    text = remove_hashtag_and_usertag(text)                       #7\n","    text = remove_rt_url_different_numeric_tag_numbers(text)      #8\n","    text = remove_punctuation(text)                               #9\n","    text = remove_extra_white_spaces(text)                        #10\n","    # text = spelling_correction(text)                              #11\n","    text = lemmatization(text)                                    #12\n","    # text = stemming(text)                                         #13\n","    text = remove_stopword(text)                                  #14\n","    corpus[i] = text\n","    # print('no: ',i)\n","  return corpus"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BDsRjiaLTFOz"},"source":["corpus = cleaning(dataset)\n","# corpus = np.array(corpus)\n","# print(corpus)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iNdSHHfhTWxm"},"source":["#Making json file of preprocessed tweets\n","corpus_file = open(\"final_data.json\", \"w\")  \n","json.dump(corpus, corpus_file, indent = 6)  \n","corpus_file.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PU516abhGqRy"},"source":["###for saving the file in google drive\n","!cp final_data.json \"/content/gdrive/My Drive/Abusive_text_detection/Preprocess\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FlGcmL6eu9f6"},"source":["#Making json file of preprocessed tweet's Labels\n","label_file = open(\"class_value.json\", \"w\")  \n","json.dump(list(dataset['abusive']), label_file, indent = 6)  \n","label_file.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZwPY-WQDAtFE"},"source":["###for saving the file in google drive\n","!cp class_value.json \"/content/gdrive/My Drive/Abusive_text_detection/Preprocess\""],"execution_count":null,"outputs":[]}]}